{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TF_BasicsTextClass.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOFtMtDDnaLv4PeiPHxqo2C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/squeeko/TensorflowProjects/blob/master/TF_BasicsTextClass.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05g_Z2a8FuG4",
        "colab_type": "text"
      },
      "source": [
        "# Basic text classification\n",
        "This tutorial demonstrates text classification starting from plain text files stored on disk. You'll train a binary classifier to perform sentiment analysis on an IMDB dataset. At the end of the notebook, there is an exercise for you to try, in which you'll train a multiclass classifier to predict the tag for a programming question on Stack Overflow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOWRsfEGFkxE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import preprocessing\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ni6Q3yV3HCvd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f3f92526-4506-47d5-b7fd-116bf9f385a5"
      },
      "source": [
        "print(tf.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQaEi58lHHFP",
        "colab_type": "text"
      },
      "source": [
        "# Sentiment analysis\n",
        "\n",
        "This notebook trains a sentiment analysis model to classify movie reviews as positive or negative, based on the text of the review. This is an example of binary—or two-class—classification, an important and widely applicable kind of machine learning problem.\n",
        "\n",
        "You'll use the Large Movie Review Dataset that contains the text of 50,000 movie reviews from the Internet Movie Database. These are split into 25,000 reviews for training and 25,000 reviews for testing. The training and testing sets are balanced, meaning they contain an equal number of positive and negative reviews."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVohyTUzIvFg",
        "colab_type": "text"
      },
      "source": [
        "## Download and explore the IMDB dataset\n",
        "Let's download and extract the dataset, then explore the directory structure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "petVDO0vHFbB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b6f9203e-414c-4bfe-b7a4-a9cff578e4e7"
      },
      "source": [
        "url = 'https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
        "dataset = tf.keras.utils.get_file(\"aclImdb_v1.tar.gz\", url,\n",
        "                                    untar=True, cache_dir='.',\n",
        "                                    cache_subdir='')\n",
        "dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "84131840/84125825 [==============================] - 6s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eSIE8-WJW_7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "466202dd-1ff5-48f4-b757-4a845f2b3fe2"
      },
      "source": [
        "os.listdir(dataset_dir)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['test', 'imdb.vocab', 'train', 'README', 'imdbEr.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1br3qE9kJhM4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "89c2165a-eede-4073-c96c-38f83eff3efd"
      },
      "source": [
        "train_dir = os.path.join(dataset_dir, 'train')\n",
        "os.listdir(train_dir)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['unsupBow.feat',\n",
              " 'urls_pos.txt',\n",
              " 'neg',\n",
              " 'urls_unsup.txt',\n",
              " 'unsup',\n",
              " 'urls_neg.txt',\n",
              " 'labeledBow.feat',\n",
              " 'pos']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LrebSxiKK9u",
        "colab_type": "text"
      },
      "source": [
        "The aclImdb/train/pos and aclImdb/train/neg directories contain many text files, each of which is a single movie review. Let's take a look at one of them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQl5A874J9oa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "337dc582-7890-4653-9b7e-8e0734049302"
      },
      "source": [
        "sample_file = os.path.join(train_dir, 'pos/1181_9.txt')\n",
        "with open(sample_file) as f:\n",
        "  print(f.read())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Rachel Griffiths writes and directs this award winning short film. A heartwarming story about coping with grief and cherishing the memory of those we've loved and lost. Although, only 15 minutes long, Griffiths manages to capture so much emotion and truth onto film in the short space of time. Bud Tingwell gives a touching performance as Will, a widower struggling to cope with his wife's death. Will is confronted by the harsh reality of loneliness and helplessness as he proceeds to take care of Ruth's pet cow, Tulip. The film displays the grief and responsibility one feels for those they have loved and lost. Good cinematography, great direction, and superbly acted. It will bring tears to all those who have lost a loved one, and survived.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCj7ZV6SKrLY",
        "colab_type": "text"
      },
      "source": [
        "## Load the dataset\n",
        "Next, you will load the data off disk and prepare it into a format suitable for training. To do so, you will use the helpful text_dataset_from_directory utility, which expects a directory structure as follows.\n",
        "\n",
        "**main_directory/**\n",
        "\n",
        "...class_a/\n",
        "\n",
        "  ......a_text_1.txt\n",
        "\n",
        "  ......a_text_2.txt\n",
        "\n",
        "...class_b/\n",
        "\n",
        "  ......b_text_1.txt\n",
        "\n",
        "  ......b_text_2.txt**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHKmRm1sLfHL",
        "colab_type": "text"
      },
      "source": [
        "To prepare a dataset for binary classification, you will need two folders on disk, corresponding to class_a and class_b. These will be the positive and negative movie reviews, which can be found in aclImdb/train/pos and aclImdb/train/neg. As the IMDB dataset contains additional folders, you will remove them before using this utility."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHtnGSsZKCX7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "remove_dir = os.path.join(train_dir, 'unsup')\n",
        "shutil.rmtree(remove_dir)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLSX9hncLvr4",
        "colab_type": "text"
      },
      "source": [
        "Next, you will use the **text_dataset_from_directory** utility to create a labeled **tf.data.Dataset**. tf.data is a powerful collection of tools for working with data.\n",
        "\n",
        "When running a machine learning experiment, it is a best practice to divide your dataset into three splits: train, validation, and test.\n",
        "\n",
        "The IMDB dataset has already been divided into train and test, but it lacks a validation set. Let's create a validation set using an 80:20 split of the training data by using the **validation_split** argument below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pgf8K4NyLruR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}